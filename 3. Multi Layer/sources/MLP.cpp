#include "MLP.h"
#include <iostream>
#include <fstream>
#include <filesystem>
#include <string>

namespace fs = std::filesystem;

// Create a perceptron with multiple layers(including input/output layers).
MLP::MLP(const int layers, const int nodesPerLayer[]) {
    // Exception handling.
    if (layers <= 1) {
        std::cerr << "MLP: MLP must has at least two layers.";
        exit(1);
    }

    this->inputDim = nodesPerLayer[0];
    this->outputDim = nodesPerLayer[layers-1];

    for (int i = 0; i < layers-1; ++i) {
        modules.push_back(new FullyConnected(nodesPerLayer[i], nodesPerLayer[i+1]));
        modules.push_back(new Sigmoid(nodesPerLayer[i+1]));
    }
}

// In destructor, delete inner modules which were dynamically allocated.
MLP::~MLP() {
    for (Module* m : modules) {
        delete m;
    }
}

// Get the number of input dimensions of this perceptron.
int MLP::getInputSize() {
    return inputDim;
}

// Get the number of output dimensions of this perceptron.
int MLP::getOutputSize() {
    return outputDim;
}

// Calculate output of a this perceptron.
// Internally, this function is implemented by calling getOutput of inner Modules repeatedly.
std::vector<double> MLP::getOutput(std::vector<double>& inputs) {
    // Exception handling.
    if (inputDim != inputs.size()) {
        std::cerr << "getOutput: The size of input vector is invalid.";
        exit(1);
    }

    // Call getOutput function sequentially.
    std::vector<double> currVector = inputs;
    for (Module* m : modules) {
        currVector = m->getOutput(currVector);
    }
    return currVector;
}

// Update learnable parameters(weights) with gradient vector which is generated by Error function.
std::vector<double> MLP::doBackpropagation(std::vector<double>& upperGradients, double learningRate) {
    // Exception handling.
    if (upperGradients.size() != outputDim) {
        std::cerr << "doBackpropagation: The size of upper gradient vector must be equal to the size of output vector.";
        exit(1);
    }
    
    // Call doBackpropagation function sequentially.
    std::vector<double> gradients = upperGradients;
    for (int i = int(modules.size())-1; i >= 0; --i) {
        gradients = modules[i]->doBackpropagation(gradients, learningRate);
    }
    return gradients;
}

bool MLP::saveWeights(std::string dirStr) {
    fs::path path = dirStr;
    fs::create_directories(path);

    int count = 0;
    for (Module* m : modules) {
        std::string filename = std::string("layer-") + std::to_string(count+1);
        if (m->saveWeights(path / filename)) {
            ++count;
        }
    }
    return true;
}