#include "Perceptron.h"
#include <iostream>

// Create a perceptron which get n-dimensional inputs and give 1 or 0 output.
Perceptron::Perceptron(int inputDim) {
    // Perceptron consists of two steps: production with weights and apply activation function.
    // Codes below implements that directly. After creating ProductLayer and StepFunction,
    // getOutput function and doBackpropagation function is implemented to use these modules sequentially.
    modules.push_back(new ProductLayer(inputDim, 1));
    modules.push_back(new StepFunction(1));
    this->inputDim = inputDim;
}

// In destructor, delete inner modules which were dynamically allocated.
Perceptron::~Perceptron() {
    for (Module* m : modules) {
        delete m;
    }
}

// Get the number of input dimensions of this perceptron.
int Perceptron::getInputSize() {
    return inputDim;
}

// Get the number of output dimensions of this perceptron (=1).
int Perceptron::getOutputSize() {
    return 1;
}

// Calculate output of a logic gate.
// Internally, this function is implemented by calling getOutput of inner Modules repeatedly.
std::vector<double> Perceptron::getOutput(std::vector<double>& inputs) {
    // Exception handling.
    if (inputDim != inputs.size()) {
        std::cerr << "getOutput: The size of input vector is invalid.";
    }

    // Call getOutput function sequentially.
    std::vector<double> currVector = inputs;
    for (Module* m : modules) {
        currVector = m->getOutput(currVector);
    }
    return currVector;
}

// Update learnable parameters(weights) with gradient vector which is generated by Error function.
std::vector<double> Perceptron::doBackpropagation(std::vector<double>& upperGradients, double learningRate) {
    // Exception handling.
    if (upperGradients.size() != 1) {
        std::cerr << "doBackpropagation: The size of upper gradient vector must be 1.";
        exit(1);
    }
    
    // Call doBackpropagation function sequentially.
    std::vector<double> gradients = upperGradients;
    for (int i = int(modules.size())-1; i >= 0; --i) {
        gradients = modules[i]->doBackpropagation(gradients, learningRate);
    }
    return gradients;
}